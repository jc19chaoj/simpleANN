{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('dla': conda)",
   "metadata": {
    "interpreter": {
     "hash": "04ac6d511467e05e0bf921b6c91086202819c09a4760412820c127091509c40a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers = [2, 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(np.random.randn(layers[i], layers[i+1]))\n",
    "            self.biases.append(np.random.randn(1, layers[i+1]))\n",
    "    \n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # return the feedforward value for x\n",
    "        a = np.copy(x)\n",
    "        z_s = []\n",
    "        a_s = [a]\n",
    "        for i in range(len(self.weights)):\n",
    "            activation_function = self.getActivationFunction(self.activations[i])\n",
    "            #z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
    "            z_s.append(a.dot(self.weights[i]) + self.biases[i])\n",
    "            a = activation_function(z_s[-1])\n",
    "            a_s.append(softmax(a))\n",
    "        return (z_s, a_s)\n",
    "\n",
    "\n",
    "    def backpropagation(self,y, z_s, a_s):\n",
    "            dw = []  # dC/dW\n",
    "            db = []  # dC/dB\n",
    "            deltas = [None] * len(self.weights)  # delta = dC/dZ  known as error for each layer\n",
    "            # insert the last layer error\n",
    "            # deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
    "            deltas[-1] = y-a_s[-1]\n",
    "\n",
    "            # Perform BackPropagation\n",
    "            for i in reversed(range(len(deltas)-1)):\n",
    "                #deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))  \n",
    "                deltas[i] = deltas[i+1].dot(self.weights[i+1].T)*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))  \n",
    "\n",
    "            #a= [print(d.shape) for d in deltas]\n",
    "            batch_size = y.shape[0]\n",
    "            # db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "            # dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "\n",
    "            db = [np.ones((1, batch_size)).dot(d)/float(batch_size) for d in deltas]\n",
    "            dw = [a_s[i].T.dot(d)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "\n",
    "            # return the derivitives respect to weight matrix and biases\n",
    "            return dw, db\n",
    "\n",
    "\n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "    # update weights and biases based on the output\n",
    "        for e in range(epochs): \n",
    "            i=0\n",
    "            while(i<len(y)):\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                i = i+batch_size\n",
    "                z_s, a_s = self.feedforward(x_batch)\n",
    "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
    "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
    "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
    "            \n",
    "            print(\"Epoch {}: loss = {}\".format(e, np.linalg.norm(a_s[-1]-y_batch)), end=\"\\n\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def getActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : np.exp(x)/(1+np.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: x\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def getDerivitiveActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = np.copy(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download fashion mnist dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X, y), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "#train_set_count = len(train_labels)\n",
    "#test_set_count = len(test_labels)\n",
    "\n",
    "#normalize images\n",
    "X = X / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X.reshape(-1, 28*28)\n",
    "y_new = tf.keras.utils.to_categorical(\n",
    "    y, num_classes=10, dtype='float32'\n",
    ")\n",
    "y_new = y_new.reshape(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 784) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_new.shape, y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: loss = 8.817486729146328\n",
      "Epoch 1: loss = 8.020484532557296\n",
      "Epoch 2: loss = 7.733619594034876\n",
      "Epoch 3: loss = 7.536883139431679\n",
      "Epoch 4: loss = 7.38507688155297\n",
      "Epoch 5: loss = 7.258183536793553\n",
      "Epoch 6: loss = 7.132207892702848\n",
      "Epoch 7: loss = 7.025584259383275\n",
      "Epoch 8: loss = 6.976699819773272\n",
      "Epoch 9: loss = 6.921042195431771\n",
      "Epoch 10: loss = 6.895732242476954\n",
      "Epoch 11: loss = 6.861076053000031\n",
      "Epoch 12: loss = 6.840520021534309\n",
      "Epoch 13: loss = 6.8212698146156105\n",
      "Epoch 14: loss = 6.796864719452753\n",
      "Epoch 15: loss = 6.776249655514505\n",
      "Epoch 16: loss = 6.7511499117818685\n",
      "Epoch 17: loss = 6.724901965436472\n",
      "Epoch 18: loss = 6.706519240519335\n",
      "Epoch 19: loss = 6.677520518958638\n",
      "Epoch 20: loss = 6.654211382449374\n",
      "Epoch 21: loss = 6.623653083611586\n",
      "Epoch 22: loss = 6.592127775536609\n",
      "Epoch 23: loss = 6.5634339765203205\n",
      "Epoch 24: loss = 6.540652352117441\n",
      "Epoch 25: loss = 6.520975761762829\n",
      "Epoch 26: loss = 6.501230932650896\n",
      "Epoch 27: loss = 6.482764245921384\n",
      "Epoch 28: loss = 6.4644966864831215\n",
      "Epoch 29: loss = 6.446582696429213\n",
      "Epoch 30: loss = 6.432692199897853\n",
      "Epoch 31: loss = 6.416968599814399\n",
      "Epoch 32: loss = 6.3996200791553814\n",
      "Epoch 33: loss = 6.385087001116624\n",
      "Epoch 34: loss = 6.371469384617996\n",
      "Epoch 35: loss = 6.362112273688798\n",
      "Epoch 36: loss = 6.351842537938547\n",
      "Epoch 37: loss = 6.344312800633027\n",
      "Epoch 38: loss = 6.339106869015\n",
      "Epoch 39: loss = 6.335868569351482\n",
      "Epoch 40: loss = 6.3305055375965145\n",
      "Epoch 41: loss = 6.32349935419837\n",
      "Epoch 42: loss = 6.314723582410328\n",
      "Epoch 43: loss = 6.304051403153696\n",
      "Epoch 44: loss = 6.293569805255466\n",
      "Epoch 45: loss = 6.284795939165195\n",
      "Epoch 46: loss = 6.275944985094616\n",
      "Epoch 47: loss = 6.262518652843904\n",
      "Epoch 48: loss = 6.249380216552263\n",
      "Epoch 49: loss = 6.236214406141028\n",
      "Epoch 50: loss = 6.22722361205787\n",
      "Epoch 51: loss = 6.21413921011031\n",
      "Epoch 52: loss = 6.201668431713719\n",
      "Epoch 53: loss = 6.191779314238527\n",
      "Epoch 54: loss = 6.178771330611525\n",
      "Epoch 55: loss = 6.164509769202575\n",
      "Epoch 56: loss = 6.1522554076753595\n",
      "Epoch 57: loss = 6.141267748666196\n",
      "Epoch 58: loss = 6.133932533612114\n",
      "Epoch 59: loss = 6.121744797921812\n",
      "Epoch 60: loss = 6.107639083135068\n",
      "Epoch 61: loss = 6.09904261825059\n",
      "Epoch 62: loss = 6.087619105516676\n",
      "Epoch 63: loss = 6.077023810399845\n",
      "Epoch 64: loss = 6.066099661975567\n",
      "Epoch 65: loss = 6.057668673903404\n",
      "Epoch 66: loss = 6.046305854289785\n",
      "Epoch 67: loss = 6.033226032513967\n",
      "Epoch 68: loss = 6.019235236000328\n",
      "Epoch 69: loss = 6.006116508526576\n",
      "Epoch 70: loss = 5.993786549179727\n",
      "Epoch 71: loss = 5.982628087922131\n",
      "Epoch 72: loss = 5.97229138408419\n",
      "Epoch 73: loss = 5.9624926331367245\n",
      "Epoch 74: loss = 5.95218570821991\n",
      "Epoch 75: loss = 5.945900402920069\n",
      "Epoch 76: loss = 5.938458194582055\n",
      "Epoch 77: loss = 5.9324440682509865\n",
      "Epoch 78: loss = 5.926235350299385\n",
      "Epoch 79: loss = 5.921850469584669\n",
      "Epoch 80: loss = 5.916137192384938\n",
      "Epoch 81: loss = 5.909365790364551\n",
      "Epoch 82: loss = 5.901427366325848\n",
      "Epoch 83: loss = 5.893221450736139\n",
      "Epoch 84: loss = 5.8868843746075905\n",
      "Epoch 85: loss = 5.879111036169755\n",
      "Epoch 86: loss = 5.8732399153455095\n",
      "Epoch 87: loss = 5.869354748643755\n",
      "Epoch 88: loss = 5.863138712003957\n",
      "Epoch 89: loss = 5.857950377232011\n",
      "Epoch 90: loss = 5.852808684521443\n",
      "Epoch 91: loss = 5.843781784458225\n",
      "Epoch 92: loss = 5.834687463134489\n",
      "Epoch 93: loss = 5.828446884363433\n",
      "Epoch 94: loss = 5.82051026826236\n",
      "Epoch 95: loss = 5.812107104744956\n",
      "Epoch 96: loss = 5.8053001036216685\n",
      "Epoch 97: loss = 5.799252086262113\n",
      "Epoch 98: loss = 5.795143814054755\n",
      "Epoch 99: loss = 5.789613529614645\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([784, 32, 10],activations=['relu', 'linear'])\n",
    "\n",
    "nn.train(X_new, y_new, epochs=100, batch_size=100, lr = .01)\n",
    "#_, a_s = nn.feedforward(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2.12062451e-04, 5.76445508e-04, 1.56694135e-03, 4.25938820e-03,\n",
       "        1.15782175e-02, 3.14728583e-02, 8.55520989e-02, 2.32554716e-01,\n",
       "        6.32149258e-01, 7.80134161e-05]])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "x=np.array([1,2,3,4,5,6,7,8,9,0]).reshape(2,10)\n",
    "\n",
    "e_x = np.exp(x-np.max(x)) \n",
    "\n",
    "e_x / e_x.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2*np.pi*np.random.rand(1000).reshape(1, -1)\n",
    "y = np.sin(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}